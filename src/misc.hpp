#include <sstream>
#include <map>
#include <iostream>
#include <fstream> 
#include <string> 
#include <stdexcept> 
#include <tuple> 
#include <iomanip> 
#include <vector> 
#include <sys/stat.h> 
#include <cerrno>
#ifdef _WIN32
#include <io.h>
#include <direct.h> 
#define access _access
#define F_OK 0
#else
#include <unistd.h>
#endif


/**
 * @brief Collects statistics of error vectors by weight.
 *
 * Processes all error vectors generated by the ISD algorithm and computes
 * the frequency of error vectors for each Hamming weight. The function merges
 * these counts into the provided statistics map, allowing incremental updates.
 *
 * @param statistics A map to store the cumulative frequency of error vectors by weight.
 *                   Keys are weights, and values are their respective counts.
 * @param errors A vector of error vectors (output of `find_errors`).
 */

void collect_statistics(std::map<std::size_t, std::pair<double, long double>>& statistics, 
    const std::vector<binop::binvec>& errors, const std::vector<long double>& predictions) {
    if (errors.empty()) {
        std::cout << "No errors to analyze.\n";
        return;
    }

    std::size_t min_weight = std::numeric_limits<std::size_t>::max(); // Initialize to max possible value

    // Iterate over all error vectors
    for (const auto& e : errors) {
        // Compute the Hamming weight of the error vector
        std::size_t weight = e.count();

        // Increment the count for this weight in the statistics map
        statistics[weight].first += 1;

        // Update the minimum weight found
        if (weight < min_weight) {
            min_weight = weight;
        }
    }

    // Iterate over all predictions
    for (size_t i = 0; i < predictions.size(); ++i) {
        statistics[i].second = predictions[i];
    }

    // Print the smallest error weight
    std::cout << "Smallest error weight: " << min_weight << std::endl;
}



/**
 * @brief Writes normalized error statistics to a text file.
 *
 * Saves the statistics of error vectors by Hamming weight to a specified file.
 * The file includes three columns: the weight (w), the normalized count of error vectors
 * with that weight (ISD_exp), and the normalized predicted value (ISD_pred).
 *
 * @param alg The name of the algorithm (used as a prefix in the headers).
 * @param dir The directory where the file will be written.
 * @param filename The name of the output file.
 * @param statistics A map where the key is the Hamming weight (w) and the value
 *                   is a pair of the experimental and predicted counts.
 * @throws std::ios_base::failure If the file cannot be opened for writing.
 */


void write_statistics_to_file(const std::string& alg, const std::string& dir, const std::string& filename,
    const std::map<std::size_t, std::pair<double, long double>>& statistics) {
    // Create the directory
    #ifdef _WIN32
        if (_mkdir(dir.c_str()) != 0 && errno != EEXIST) {
            throw std::runtime_error("Failed to create directory: " + dir);
        }
    #else
        if (mkdir(dir.c_str(), 0755) != 0 && errno != EEXIST) {
            throw std::runtime_error("Failed to create directory: " + dir);
        }
    #endif

    // Construct the full file path
    std::string filepath = dir + "/" + filename;

    // Open the file
    std::ofstream file(filepath);
    if (!file.is_open()) {
        throw std::ios_base::failure("Failed to open file: " + filepath);
    }

    // Write the header
    file << "w\t" + alg + "_exp\t" + alg + "_pred\n";

    // Write each weight and count to the file
    for (auto& entry : statistics) {
        file << entry.first << "\t"
             << std::scientific << std::setprecision(3) << entry.second.first << "\t" << entry.second.second << "\n";
    }

    file.close();
}